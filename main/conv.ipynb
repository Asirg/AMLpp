{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(\r\n",
    "    {'fea1':[0,0,0,2,2,2,1,1,1],\r\n",
    "     'fea2':['a',np.nan,'b','b','b','b',np.nan,'c','c'],\r\n",
    "     'fea3':['Магазин','Магазин1','Магази','Что','ЧТОт','ЧТК','ЧАШК','ЧАШКК','чаш']})\r\n",
    "Y = pd.DataFrame({'target':[1,1,1,1,0,1,0,0,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(\r\n",
    "    {'fea1':[0,0,1,2,5],\r\n",
    "     'fea2':['a',np.nan,'b',np.nan,'c'],\r\n",
    "     'fea3':['1','1','1','1','1']})\r\n",
    "Y_test = pd.DataFrame({'target':[1,1,1,1,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.сonveyor import *\r\n",
    "from sklearn.inspection import permutation_importance\r\n",
    "from sklearn.preprocessing import OrdinalEncoder\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from typing import List, Callable\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import time\r\n",
    "import shap\r\n",
    "####################################################################################################\r\n",
    "class Column_scaler:\r\n",
    "    \"\"\"\r\n",
    "    Pipeline of transforms with a final estimator.\r\n",
    "\r\n",
    "    Sequentially apply a list of transforms and a final estimator.\r\n",
    "    Intermediate steps of the pipeline must be 'transforms', that is, they\r\n",
    "    must implement fit and transform methods.\r\n",
    "    The final estimator only needs to implement fit.\r\n",
    "    The transformers in the pipeline can be cached using ``memory`` argument.\r\n",
    "\r\n",
    "    The purpose of the pipeline is to assemble several steps that can be\r\n",
    "    cross-validated together while setting different parameters.\r\n",
    "    For this, it enables setting parameters of the various steps using their\r\n",
    "    names and the parameter name separated by a '__', as in the example below.\r\n",
    "    A step's estimator may be replaced entirely by setting the parameter\r\n",
    "    with its name to another estimator, or a transformer removed by setting\r\n",
    "    it to 'passthrough' or ``None``.\r\n",
    "\r\n",
    "    Read more in the :ref:`User Guide <pipeline>`.\r\n",
    "\r\n",
    "    .. versionadded:: 0.5\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    steps : list\r\n",
    "        List of (name, transform) tuples (implementing fit/transform) that are\r\n",
    "        chained, in the order in which they are chained, with the last object\r\n",
    "        an estimator.\r\n",
    "\r\n",
    "    memory : str or object with the joblib.Memory interface, default=None\r\n",
    "        Used to cache the fitted transformers of the pipeline. By default,\r\n",
    "        no caching is performed. If a string is given, it is the path to\r\n",
    "        the caching directory. Enabling caching triggers a clone of\r\n",
    "        the transformers before fitting. Therefore, the transformer\r\n",
    "        instance given to the pipeline cannot be inspected\r\n",
    "        directly. Use the attribute ``named_steps`` or ``steps`` to\r\n",
    "        inspect estimators within the pipeline. Caching the\r\n",
    "        transformers is advantageous when fitting is time consuming.\r\n",
    "\r\n",
    "    verbose : bool, default=False\r\n",
    "        If True, the time elapsed while fitting each step will be printed as it\r\n",
    "        is completed.\r\n",
    "\r\n",
    "    Attributes\r\n",
    "    ----------\r\n",
    "    named_steps : :class:`~sklearn.utils.Bunch`\r\n",
    "        Dictionary-like object, with the following attributes.\r\n",
    "        Read-only attribute to access any step parameter by user given name.\r\n",
    "        Keys are step names and values are steps parameters.\r\n",
    "\r\n",
    "    See Also\r\n",
    "    --------\r\n",
    "    make_pipeline : Convenience function for simplified pipeline construction.\r\n",
    "\r\n",
    "    Examples\r\n",
    "    --------\r\n",
    "    >>> from sklearn.svm import SVC\r\n",
    "    >>> from sklearn.preprocessing import StandardScaler\r\n",
    "    >>> from sklearn.datasets import make_classification\r\n",
    "    >>> from sklearn.model_selection import train_test_split\r\n",
    "    >>> from sklearn.pipeline import Pipeline\r\n",
    "    >>> X, y = make_classification(random_state=0)\r\n",
    "    >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\r\n",
    "    ...                                                     random_state=0)\r\n",
    "    >>> pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\r\n",
    "    >>> # The pipeline can be used as any other estimator\r\n",
    "    >>> # and avoids leaking the test set into the train set\r\n",
    "    >>> pipe.fit(X_train, y_train)\r\n",
    "    Pipeline(steps=[('scaler', StandardScaler()), ('svc', SVC())])\r\n",
    "    >>> pipe.score(X_test, y_test)\r\n",
    "    0.88\r\n",
    "    \"\"\"\r\n",
    "    ordinal_scaler = {}\r\n",
    "    means_scaler = {}\r\n",
    "\r\n",
    "    def __init__(self, columns_to_ordinal, columns_to_int):\r\n",
    "        self.columns_to_ordinal = columns_to_ordinal\r\n",
    "        self.columns_to_int = columns_to_int\r\n",
    "\r\n",
    "    ## Выборка слов из датасета, и подача их на вход \r\n",
    "    def fit(self, X:pd.DataFrame, y:pd.Series):\r\n",
    "        \"\"\"Fit the model and transform with the final estimator\r\n",
    "\r\n",
    "        Fits all the transforms one after the other and transforms the\r\n",
    "        data, then uses fit_transform on transformed data with the final\r\n",
    "        estimator.\r\n",
    "\r\n",
    "        Parameters\r\n",
    "        ----------\r\n",
    "        X : iterable\r\n",
    "            Training data. Must fulfill input requirements of first step of the\r\n",
    "            pipeline.\r\n",
    "\r\n",
    "        y : iterable, default=None\r\n",
    "            Training targets. Must fulfill label requirements for all steps of\r\n",
    "            the pipeline.\r\n",
    "\r\n",
    "        **fit_params : dict of string -> object\r\n",
    "            Parameters passed to the ``fit`` method of each step, where\r\n",
    "            each parameter name is prefixed such that parameter ``p`` for step\r\n",
    "            ``s`` has key ``s__p``.\r\n",
    "\r\n",
    "        Returns\r\n",
    "        -------\r\n",
    "        Xt : array-like of shape  (n_samples, n_transformed_features)\r\n",
    "            Transformed samples\r\n",
    "        \"\"\"\r\n",
    "        for column in self.columns_to_ordinal:\r\n",
    "            self.ordinal_scaler[column] = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\r\n",
    "            self.ordinal_scaler[column].fit(X[column].to_frame().fillna('NAN').astype(str))\r\n",
    "            ordinal_code = self.to_ordinal_scaler(X[column])\r\n",
    "            self.means_scaler[column] = np.mean(ordinal_code[~np.isnan(ordinal_code)])\r\n",
    "            \r\n",
    "        for column in self.columns_to_int:\r\n",
    "            self.means_scaler[column] = float(X[column].loc[~X[column].isnull()].mean())\r\n",
    "\r\n",
    "\r\n",
    "        if not os.path.exists('model_new_property'):\r\n",
    "            os.makedirs('model_new_property')\r\n",
    "        with open('model_new_property/means_scaler', 'wb') as file:\r\n",
    "            pickle.dump(self.means_scaler,file)\r\n",
    "        with open('model_new_property/ordinal_scaler', 'wb') as file:\r\n",
    "            pickle.dump(self.ordinal_scaler,file)\r\n",
    "        return self\r\n",
    "\r\n",
    "    def transform(self, X:pd.DataFrame, y = None)->pd.Series:\r\n",
    "        with open('model_new_property/means_scaler', 'rb') as file:\r\n",
    "            self.means_scaler = pickle.load(file)\r\n",
    "        with open('model_new_property/ordinal_scaler', 'rb') as file:\r\n",
    "            self.ordinal_scaler = pickle.load(file)\r\n",
    "\r\n",
    "        for column in self.columns_to_ordinal:\r\n",
    "            X[column] = self.to_ordinal_scaler(X[column])\r\n",
    "            X[column].loc[X[column].isnull()] = self.means_scaler[column]\r\n",
    "            \r\n",
    "        for column in self.columns_to_int:\r\n",
    "            X[column] = self.to_int_with_mean(X[column])\r\n",
    "            X[column].loc[X[column].isnull()] = self.means_scaler[column]\r\n",
    "\r\n",
    "        for column in X:\r\n",
    "            X[column].loc[X[column].isnull()] = 0\r\n",
    "        X['purpose_other'] =  X['purpose_other'].astype(str)\r\n",
    "        return X\r\n",
    "        \r\n",
    "\r\n",
    "    def to_ordinal_scaler(self, series: pd.Series) -> pd.Series:\r\n",
    "        frame = series.to_frame().fillna('NAN')\r\n",
    "        frame = series.astype(str)\r\n",
    "        return self.ordinal_scaler[series.name].transform(frame.values.reshape(-1,1))\r\n",
    "\r\n",
    "    def to_int_with_mean(self, series: pd.Series) -> pd.Series:\r\n",
    "        try:\r\n",
    "            series = series.fillna(self.means_scaler[series.name])\r\n",
    "            series = series.astype(int)\r\n",
    "        except Exception as e:\r\n",
    "            series = series.fillna(0)\r\n",
    "            series = series.astype(int)\r\n",
    "        return series\r\n",
    "\r\n",
    "class Сap():\r\n",
    "\r\n",
    "    regr = []\r\n",
    "\r\n",
    "    def __init__(self):\r\n",
    "        self.data = 1\r\n",
    "\r\n",
    "    def fit(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series):\r\n",
    "        # for iter, val in X.iterrows():\r\n",
    "        #     self.regr.append(sum([i for i in val]))\r\n",
    "        for i in X:\r\n",
    "            self.regr.append(sum(i))\r\n",
    "        return self\r\n",
    "\r\n",
    "    def transform(self, X:pd.DataFrame):\r\n",
    "        return X\r\n",
    "\r\n",
    "    def predict(self, X:pd.DataFrame):\r\n",
    "        return self.regr\r\n",
    "############################################################################################################\r\n",
    "\r\n",
    "def most_frequency(x:List[List[int]]):\r\n",
    "    x_ = [i[0] for i in x]\r\n",
    "    return np.argmax(np.bincount(x_))\r\n",
    "\r\n",
    "def lead_time(func):\r\n",
    "    def wrapper(*args, **kwargs):\r\n",
    "        start_time = time.time()\r\n",
    "        result =  func(*args, **kwargs)\r\n",
    "        print('lead time {} = {:.3f}'.format(func.__name__, time.time() - start_time))\r\n",
    "        return result \r\n",
    "    return wrapper\r\n",
    "############################################################################################################\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "class Conveyor:\r\n",
    "    \"\"\" Подобие sklearn.Pipeline, адаптированный под простоту и добавленный функционал\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    *block : object\r\n",
    "        Объекты классов, что будут использоваться при обработке, и моделирование\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    ################################################################\r\n",
    "    def __init__(self, *blocks, **params):\r\n",
    "        self.blocks = list(blocks)\r\n",
    "    \r\n",
    "    @lead_time\r\n",
    "    def fit(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series):\r\n",
    "        X_, Y_  = (X.copy(), Y.copy())\r\n",
    "        for block in self.blocks[:-1]:\r\n",
    "            block.fit(X_, Y_)\r\n",
    "            X_, Y_ = self._transform(block, X_, Y_)\r\n",
    "        self.blocks[-1].fit(X_, Y_)\r\n",
    "        return X_, Y_\r\n",
    "        \r\n",
    "    ################################################################\r\n",
    "    @lead_time\r\n",
    "    def transform(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series = pd.DataFrame()):\r\n",
    "        X_, Y_  = (X.copy(), Y.copy())\r\n",
    "        for block in self.blocks[:-1]:\r\n",
    "            X_, Y_ = self._transform(block, X_, Y_)\r\n",
    "        return X_, Y_\r\n",
    "\r\n",
    "    def _transform(self, block, X:pd.DataFrame, Y:pd.DataFrame or pd.Series = pd.DataFrame()):\r\n",
    "        X = block.transform(X)\r\n",
    "        if not Y.empty and 'target_transform' in dir(block):\r\n",
    "            Y = block.target_transform(Y)\r\n",
    "        return X, Y\r\n",
    "\r\n",
    "    ################################################################\r\n",
    "\r\n",
    "    @lead_time\r\n",
    "    def predict(self, X:pd.DataFrame):\r\n",
    "        return self.blocks[-1].predict(self.transform(X.copy())[0])\r\n",
    "\r\n",
    "    ################################################################\r\n",
    "    @lead_time\r\n",
    "    def score(self,\r\n",
    "                X:pd.DataFrame,\r\n",
    "                Y:pd.DataFrame or pd.Series,\r\n",
    "                sklearn_function:List[str] = ['roc_auc_score', 'r2_score', 'accuracy_score'],\r\n",
    "                precision_function:List[Callable] = []):\r\n",
    "\r\n",
    "        X_, Y_ = self.transform(X.copy(), Y.copy())\r\n",
    "        result = self.blocks[-1].predict(X_)\r\n",
    "\r\n",
    "        for func in sklearn_function:\r\n",
    "            try:\r\n",
    "                exec('from sklearn.metrics import ' + func)\r\n",
    "                print(\"function - {} = \".format(func), eval(\"{}(result, Y_)\".format(func)))\r\n",
    "            except Exception as e:\r\n",
    "                print(\"function - {} = ERROR: {}\".format(func, e))\r\n",
    "        for func in precision_function:\r\n",
    "            try:\r\n",
    "                print(\"function - {} = \".format(func.__name__), func(result, Y_))\r\n",
    "            except Exception as e:\r\n",
    "                print(\"function - {} = ERROR: {}\".format(func.__name__, e))\r\n",
    "    @lead_time\r\n",
    "    def feature_importances(self,\r\n",
    "                            X:pd.DataFrame,\r\n",
    "                            Y:pd.DataFrame or pd.Series, show:str = 'all'): # all, sklearn, shap\r\n",
    "                            \r\n",
    "        X_, Y_ = self.transform(X.copy(), Y.copy())\r\n",
    "        estimator = self.blocks[-1][-1] if type(self.blocks[-1]) == Pipeline else self.blocks[-1]\r\n",
    "\r\n",
    "        if show == 'all' or show == 'shap':\r\n",
    "            explainer = shap.Explainer(estimator)\r\n",
    "            shap_values = explainer(X_)\r\n",
    "            shap.plots.bar(shap_values[0])\r\n",
    "\r\n",
    "        if show == \"all\" or show == \"sklearn\":\r\n",
    "            try:\r\n",
    "                result = permutation_importance(estimator, X_, Y_, n_repeats=2, random_state=42)\r\n",
    "                index = X_.columns if type(X_) == pd.DataFrame else X.columns\r\n",
    "                forest_importances = pd.Series(result.importances_mean, index=index)\r\n",
    "                fig, ax = plt.subplots(figsize=(20, 10))\r\n",
    "                forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\r\n",
    "                ax.set_title(\"Feature importances using permutation on full model\")\r\n",
    "                ax.set_ylabel(\"Mean accuracy decrease\")\r\n",
    "                fig.tight_layout()\r\n",
    "                plt.show()\r\n",
    "            except Exception as e:\r\n",
    "                print('Sklearn plot - ERROR: ' + e)\r\n",
    "    ################################################################\r\n",
    "    @lead_time\r\n",
    "    def fit_model(self):\r\n",
    "\r\n",
    "    ################################################################\r\n",
    "    @lead_time\r\n",
    "    def export(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "class CategoricalEncoder():\r\n",
    "    \"\"\" Класс кодирования категориальных данных, с заполнение пропусков на некоторое значение определенное сратегией\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    columns : List[str]\r\n",
    "        Названия столбцов, которые будут подвегнуты обработке\r\n",
    "\r\n",
    "    straegy : str\r\n",
    "        Строка указывающая на используемую стратегию заполнения пропусков\r\n",
    "    \r\n",
    "    fill_value : float or str\r\n",
    "        Заполнитель, которым будут заполняться пропущенные значения,\r\n",
    "        при использование стратегии const\r\n",
    "        \r\n",
    "    \"\"\"\r\n",
    "    encoder = {}\r\n",
    "\r\n",
    "    def __init__(self, columns:List[str], strategy:str='mean', fill_value:float or str = np.nan): # strategy in mean, median, most_frequency, const, iterative inputer?\r\n",
    "        self.columns = columns\r\n",
    "        self.fill_value = {'mean':np.mean, 'median':np.median, 'most_freq':most_frequency, 'const':(lambda x:fill_value)}\r\n",
    "        self.fill_value = self.fill_value[strategy]\r\n",
    "\r\n",
    "    def fit(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series):\r\n",
    "        for column in self.columns:\r\n",
    "            self.encoder[column] = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\r\n",
    "            X_fit = pd.DataFrame(X[column].loc[~X[column].isnull()])\r\n",
    "            self.encoder[column].fit(X_fit)\r\n",
    "            X_transform = self.encoder[column].transform(pd.DataFrame(X_fit))\r\n",
    "            self.encoder[column].unknown_value = self.fill_value(X_transform)\r\n",
    "        return self\r\n",
    "\r\n",
    "    def transform(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series = None):\r\n",
    "        for column in self.columns:\r\n",
    "            X[column] = self.encoder[column].transform(pd.DataFrame(X[column].fillna('NAN')))\r\n",
    "        return X\r\n",
    "\r\n",
    "# class Imputer():\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [],
   "source": [
    "class user_transform():\r\n",
    "    def __init__(self):\r\n",
    "        self.data = 1\r\n",
    "\r\n",
    "    def fit(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series):\r\n",
    "        return self\r\n",
    "\r\n",
    "    def transform(self, X:pd.DataFrame):\r\n",
    "        return X\r\n",
    "    \r\n",
    "    def target_transform(self, Y:pd.DataFrame or pd.Series):\r\n",
    "        Y['target'] = [Y.loc[i, 'target']*-1 for i in range(len(Y['target']))]\r\n",
    "        return Y\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\r\n",
    "from sklearn.pipeline import make_pipeline\r\n",
    "\r\n",
    "pipe1 = make_pipeline(StandardScaler())\r\n",
    "pipe2 = make_pipeline(RandomForestRegressor())\r\n",
    "\r\n",
    "model = Conveyor(user_transform(),CategoricalEncoder(columns=['fea2','fea3']),pipe1,pipe2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead time fit = 0.145\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y)\r\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function - roc_auc_score = ERROR: continuous format is not supported\n",
      "function - r2_score =  -5.867418899858957\n",
      "function - accuracy_score = ERROR: Classification metrics can't handle a mix of continuous and binary targets\n"
     ]
    }
   ],
   "source": [
    "model.score(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}