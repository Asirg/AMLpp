{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import sys\r\n",
    "import pandas as pd\r\n",
    "import pickle\r\n",
    "from AMLpp.transformers import *\r\n",
    "from AMLpp.conveyor import *\r\n",
    "from AMLpp.architect import *"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\analytic6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\analytic6\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df = pd.read_excel('test.xlsx')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import sys\r\n",
    "sys.path.insert(0,'C:\\\\Users\\\\analytic6\\\\Desktop\\\\Work Space Analitic 6 (Asir)')\r\n",
    "sys.path.insert(0,'C:\\\\Users\\\\User\\\\Desktop\\\\work')\r\n",
    "\r\n",
    "from AMLpp.conveyor import Conveyor\r\n",
    "\r\n",
    "from typing import List\r\n",
    "import pandas as pd\r\n",
    "import pickle \r\n",
    "import os\r\n",
    "\r\n",
    "class Experimenter():\r\n",
    "\r\n",
    "    def __init__(self, experiment:str):\r\n",
    "        self.path_experiment = \"experiments/\" + experiment\r\n",
    "        if not os.path.exists(self.path_experiment):\r\n",
    "            os.makedirs(self.path_experiment)\r\n",
    "            self.model = None\r\n",
    "        else:\r\n",
    "            self.model = self._load_model()\r\n",
    "            print(\"load model successful!\" if self.model else \"model not found!\")\r\n",
    "\r\n",
    "    def create_experiment(self, \r\n",
    "                            model:Conveyor, \r\n",
    "                            description:str,\r\n",
    "                            trainset:str, \r\n",
    "                            X_test:pd.DataFrame = None, \r\n",
    "                            Y_test:pd.DataFrame = None,\r\n",
    "                            testset_name:str = \"\",\r\n",
    "                            feature_importances:bool = True,\r\n",
    "                            X_test_features:List[str] = None):\r\n",
    "\r\n",
    "        with open(self.path_experiment + \"/model\", 'wb') as file:\r\n",
    "            pickle.dump(model, file)\r\n",
    "        self.model = model\r\n",
    "        description += \"\\ntrainset = {}\".format(trainset)\r\n",
    "        description +=  \"\\n\" + repr(self.model)\r\n",
    "        self.add_description(description, 'w')\r\n",
    "        if type(X_test) == pd.DataFrame:\r\n",
    "            self.make_experiment(X_test, Y_test, testset_name,\r\n",
    "                        feature_importances = feature_importances, X_test_features = X_test_features)\r\n",
    "\r\n",
    "        \r\n",
    "    def make_experiment(self, \r\n",
    "                            X_test:pd.DataFrame,\r\n",
    "                            Y_test:pd.DataFrame = None, \r\n",
    "                            testset_name:str = \"\", \r\n",
    "                            add_description:str = \"\", \r\n",
    "                            feature_importances:bool = True,\r\n",
    "                            X_test_features:List[str] = None):\r\n",
    "        if self.model:\r\n",
    "            score, pred, Y = self.model.score(X_test, Y_test, _return = True) \r\n",
    "            description =  '\\n' +\"*\"*60\r\n",
    "            description += \"\\ntestset = \" + testset_name\r\n",
    "            description += \"\\n\" + score\r\n",
    "            description += add_description\r\n",
    "            self.add_description(description)\r\n",
    "            print(description)\r\n",
    "\r\n",
    "            result_data = X_test[X_test_features] if X_test_features else pd.DataFrame()\r\n",
    "            result_data['target'] = Y\r\n",
    "            result_data['result'] = pred\r\n",
    "            result_data.to_excel(self.path_experiment + \"/{}.xlsx\".format(testset_name))\r\n",
    "            \r\n",
    "            if feature_importances:\r\n",
    "                plot_path = self.path_experiment + \"/{}.jpeg\".format(testset_name)\r\n",
    "                self.model.feature_importances(X_test, Y_test, save = True, name_plot = plot_path)\r\n",
    "        else:\r\n",
    "            print(\"You need to start to the experiment !\")\r\n",
    "            print(\"Connect to existing experimnet or create experiment !\")\r\n",
    "\r\n",
    "    def add_description(self, add_description:str, mod:str = \"a\"):\r\n",
    "        with open(self.path_experiment + \"/desc.txt\", mod, encoding=\"utf-8\") as file:\r\n",
    "            file.write(add_description)\r\n",
    "\r\n",
    "    def _load_model(self) -> Conveyor:\r\n",
    "        path_model = self.path_experiment + \"/model\"\r\n",
    "        if os.path.exists(path_model):\r\n",
    "             with open(path_model, 'rb') as file:\r\n",
    "                    return pickle.load(file)\r\n",
    "        else:\r\n",
    "            return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "from sklearn.inspection import permutation_importance\r\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\r\n",
    "\r\n",
    "import sys\r\n",
    "sys.path.insert(0,'C:\\\\Users\\\\analytic6\\\\Desktop\\\\Work Space Analitic 6 (Asir)')\r\n",
    "sys.path.insert(0,'C:\\\\Users\\\\User\\\\Desktop\\\\work')\r\n",
    "\r\n",
    "from typing import List, Callable\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from tpot import TPOTRegressor\r\n",
    "\r\n",
    "from matplotlib.pyplot import figure\r\n",
    "\r\n",
    "from datetime import datetime\r\n",
    "import pandas as pd\r\n",
    "import warnings\r\n",
    "import pickle\r\n",
    "import shap\r\n",
    "\r\n",
    "import tqdm \r\n",
    "\r\n",
    "##############################################################################\r\n",
    "class Conveyor:\r\n",
    "    \"\"\" Подобие sklearn.Pipeline, адаптированный под простоту и добавленный функционал\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    *block : object\r\n",
    "        Объекты классов, что будут использоваться при обработке, и моделирование\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    ##############################################################################\r\n",
    "    def __init__(self, *blocks, **params):\r\n",
    "        self.blocks = list(blocks) # Список трансформаторов\r\n",
    "        self.iter = 0              # Итератор для представления класса как итерируемого\r\n",
    "        warnings.filterwarnings('ignore')\r\n",
    "    \r\n",
    "    def __repr__(self):\r\n",
    "        _repr = self.__class__.__name__ + \"= (\\n\"\r\n",
    "        indent = \" \" * (len(_repr) - 1)\r\n",
    "        for block in self.blocks:\r\n",
    "            _repr += \"{}{}, \\n\".format(indent, repr(block))\r\n",
    "        _repr = _repr[:-3] + \"\\n{} )\".format(indent)\r\n",
    "        return _repr\r\n",
    "\r\n",
    "    def __next__(self):\r\n",
    "        if self.iter < len(self.blocks):\r\n",
    "            self.iter +=1 \r\n",
    "            return self.block[iter]\r\n",
    "        else:\r\n",
    "            self.iter = 0\r\n",
    "            return StopIteration\r\n",
    "\r\n",
    "    def __getitem__(self, key):\r\n",
    "        if isinstance(key, slice):\r\n",
    "            return self.__class__(self.blocks[key])\r\n",
    "        else:\r\n",
    "            return self.blocks[key]\r\n",
    "    ##############################################################################\r\n",
    "    def fit(self, X:pd.DataFrame,\r\n",
    "                  Y:pd.DataFrame or pd.Series,\r\n",
    "                  feature_importances:str = False):\r\n",
    "        self._fit(X, Y)\r\n",
    "        if feature_importances:\r\n",
    "            self.feature_importances(X, Y, transform = False)\r\n",
    "\r\n",
    "    def fit_transform(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series):\r\n",
    "        X_, Y_  = (X.copy(), Y.copy())\r\n",
    "        for block in self.blocks:\r\n",
    "            block.fit(X_, Y_)\r\n",
    "            X_, Y_ = self._transform(block, X_, Y_)\r\n",
    "        return X_, Y_\r\n",
    "\r\n",
    "    def _fit(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series):\r\n",
    "        X_, Y_  = (X.copy(), Y.copy())\r\n",
    "\r\n",
    "        pbar = tqdm.tqdm(self.blocks[:-1])\r\n",
    "        for block in pbar:\r\n",
    "            pbar.set_postfix({'transform': block.__class__.__name__})\r\n",
    "            block.fit(X_, Y_)\r\n",
    "            X_, Y_ = self._transform(block, X_, Y_)\r\n",
    "        pbar.close()\r\n",
    "        \r\n",
    "        self.blocks[-1].fit(X_, Y_)\r\n",
    "        return X_, Y_\r\n",
    "    ##############################################################################\r\n",
    "    def transform(self,\r\n",
    "                        X:pd.DataFrame,\r\n",
    "                        Y:pd.DataFrame or pd.Series = pd.DataFrame()):\r\n",
    "        X_, Y_  = (X.copy(), Y.copy())\r\n",
    "        for block in self.blocks[:-1]:\r\n",
    "            X_, Y_ = self._transform(block, X_, Y_)\r\n",
    "        return X_, Y_\r\n",
    "\r\n",
    "    def _transform(self, \r\n",
    "                        block:Callable,\r\n",
    "                        X:pd.DataFrame,\r\n",
    "                        Y:pd.DataFrame or pd.Series = pd.DataFrame()):\r\n",
    "        X = block.transform(X)\r\n",
    "        if not Y.empty and 'target_transform' in dir(block):\r\n",
    "            Y = block.target_transform(Y)\r\n",
    "        return X, Y\r\n",
    "    ##############################################################################\r\n",
    "    def predict(self, X:pd.DataFrame):\r\n",
    "        X_, Y_ = self.transform(X.copy())\r\n",
    "        return self.blocks[-1].predict(X_)\r\n",
    "    ##############################################################################\r\n",
    "    # @lead_time\r\n",
    "    def score(self,\r\n",
    "                X:pd.DataFrame,\r\n",
    "                Y:pd.DataFrame or pd.Series,\r\n",
    "                sklearn_function:List[str] = ['roc_auc_score', 'r2_score', 'accuracy_score'],\r\n",
    "                precision_function:List[Callable] = [],\r\n",
    "                _return:bool = False):\r\n",
    "        \"\"\"\r\n",
    "        X:pd.DataFrame,\r\n",
    "        Y:pd.DataFrame or pd.Series,\r\n",
    "        sklearn_function:List[str] = ['roc_auc_score', 'r2_score', 'accuracy_score'],\r\n",
    "        precision_function:List[Callable] = []\r\n",
    "        \"\"\"\r\n",
    "        X_, Y_ = self.transform(X.copy(), Y.copy())\r\n",
    "        result = self.blocks[-1].predict(X_)\r\n",
    "        score = \"\"\r\n",
    "        for func in sklearn_function:\r\n",
    "            try:\r\n",
    "                exec('from sklearn.metrics import ' + func)\r\n",
    "                score += \"function - {} = {}\\n\".format(func, eval(\"{}(Y_, result)\".format(func)))\r\n",
    "            except Exception as e:\r\n",
    "                score += \"function - {} = ERROR: {}\\n\".format(func, e)\r\n",
    "        for func in precision_function:\r\n",
    "            try:\r\n",
    "                score = \"function - {} = {}\\n\".format(func.__name__, func(Y_, result))\r\n",
    "            except Exception as e:\r\n",
    "                score = \"function - {} = ERROR: {}\\n\".format(func.__name__, e)\r\n",
    "\r\n",
    "        if _return:\r\n",
    "            return score, result, Y_\r\n",
    "        else:\r\n",
    "            print(score)\r\n",
    "\r\n",
    "    def feature_importances(self,\r\n",
    "                            X:pd.DataFrame,\r\n",
    "                            Y:pd.DataFrame or pd.Series, \r\n",
    "                            show:str = 'all', # all, sklearn, shap\r\n",
    "                            save:bool = True,\r\n",
    "                            name_plot:str = \"\",\r\n",
    "                            transform = True): \r\n",
    "                            \r\n",
    "        if transform:\r\n",
    "            X_, Y_ = self.transform(X.copy(), Y.copy())\r\n",
    "            estimator = self.blocks[-1][-1] if type(self.blocks[-1]) == Pipeline else self.blocks[-1]\r\n",
    "\r\n",
    "        if show == 'all' or show == 'shap':\r\n",
    "            try:\r\n",
    "                explainer = shap.Explainer(estimator)\r\n",
    "                shap_values = explainer(X_)\r\n",
    "                \r\n",
    "                shap.plots.bar(shap_values[0], show = False)\r\n",
    "                if save:\r\n",
    "                    name_plot = name_plot if name_plot != \"\" else datetime.now().strftime(\"%Y-%m-%d_%M\")\r\n",
    "                    plt.savefig('{}_shap.jpeg'.format(name_plot), dpi = 150,  pad_inches=0)\r\n",
    "                plt.show()\r\n",
    "            except Exception as e:\r\n",
    "                print('shap plot - ERROR: ', e)\r\n",
    "\r\n",
    "        if show == \"all\" or show == \"sklearn\":\r\n",
    "            try:\r\n",
    "                result = permutation_importance(estimator, X_, Y_, n_repeats=2, random_state=42)\r\n",
    "                index = X_.columns if type(X_) == pd.DataFrame else X.columns\r\n",
    "                forest_importances = pd.Series(result.importances_mean, index=index)\r\n",
    "                fig, ax = plt.subplots(figsize=(20, 10))\r\n",
    "                forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\r\n",
    "                ax.set_title(\"Feature importances using permutation on full model\")\r\n",
    "                ax.set_ylabel(\"Mean accuracy decrease\")\r\n",
    "                fig.tight_layout()\r\n",
    "                if save:\r\n",
    "                    name_plot = name_plot if name_plot != \"\" else datetime.now().strftime(\"%Y-%m-%d_%M\")\r\n",
    "                    plt.savefig('{}_sklearn.jpeg'.format(name_plot))\r\n",
    "                plt.show()\r\n",
    "            except Exception as e:\r\n",
    "                print('Sklearn plot - ERROR: ', e)\r\n",
    "    ##############################################################################\r\n",
    "    def fit_model(self, \r\n",
    "                    X:pd.DataFrame, Y:pd.DataFrame or pd.Series,\r\n",
    "                    type_model:str = 'regressor',\r\n",
    "                    estimator:bool = True,\r\n",
    "                    only_estimator:bool = True,\r\n",
    "                    export_model:str = \"default\",\r\n",
    "                    show_best_estimators:bool = True,\r\n",
    "                    generations:int = 5, population_size:int = 50, n_jobs:int = -1):\r\n",
    "\r\n",
    "        tpot = TPOTRegressor(generations=generations, \r\n",
    "                             population_size=population_size,\r\n",
    "                             n_jobs = n_jobs,\r\n",
    "                             random_state=42)\r\n",
    "                            \r\n",
    "        X_, Y_ = self.fit_transform(X, Y) if not estimator else self._fit(X, Y)\r\n",
    "        print('start fit model !!!!')\r\n",
    "        tpot.fit(X_, Y_)\r\n",
    "        make_pipe, import_libs = tpot.export('', get_pipeline=True)\r\n",
    "\r\n",
    "        exec(import_libs)\r\n",
    "        tpot_model = eval(make_pipe)\r\n",
    "        tpot_model = tpot_model if (type(tpot_model) == Pipeline) else make_pipeline(tpot_model)\r\n",
    "        if show_best_estimators:\r\n",
    "            print('BEST TPOT:\\n' + str(tpot_model))\r\n",
    "\r\n",
    "        if estimator:\r\n",
    "            del self.blocks[-1]\r\n",
    "        \r\n",
    "        for step in tpot_model if not only_estimator else tpot_model[-1:]:\r\n",
    "            self.blocks.append(step)\r\n",
    "            self.blocks[-1].fit(X_, Y_)\r\n",
    "            if step != tpot_model[-1]:\r\n",
    "                X_, Y_ = self._transform(self.blocks[-1], X_, Y_)\r\n",
    "            \r\n",
    "        self.blocks[-1].fit(X_, Y_)\r\n",
    "        if show_best_estimators:\r\n",
    "            print('RESULT CONVEYOR:\\n'  + str(self))\r\n",
    "        if export_model != \"\":\r\n",
    "            if export_model == \"default\":\r\n",
    "                export_model = \"model_\" + datetime.now().strftime(\"%Y_%m_%d_m%M\")\r\n",
    "            with open(export_model, 'wb') as save_file:\r\n",
    "                pickle.dump(self, save_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "from experiments.exp1.user_transform import UserTransform\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "categorical_columns = ['organization_type_other', 'position_other', 'email', 'ceduc', 'family', 'sstate', 'cgrag', 'income_source_id',\r\n",
    "                      'income_frequency_id', 'has_prior_employment', 'empoyees_count_id', 'organization_branch_id', 'organization_type_id', 'position_id',\r\n",
    "                      'employment_type_id', 'has_movables', 'has_immovables', 'fact_addr_owner_type_id', 'fact_addr_region_id', 'fact_addr_same', 'addr_owner_type_id',\r\n",
    "                      'addr_region_id', 'education_id', 'children_count_id', 'marital_status_id', 'gender_id', \r\n",
    "                      'country_det','city_det', 'region_det', 'isp', 'browser', 'system', 'brand'\r\n",
    "                      ]\r\n",
    "\r\n",
    "model = Conveyor (\r\n",
    "                  UserTransform(),\r\n",
    "                  Word2Vectorization(columns=['purpose_other'], epochs = 100),\r\n",
    "                  CategoricalEncoder(columns=categorical_columns),\r\n",
    "                  ImputerIterative(),\r\n",
    "                  RandomForestRegressor(random_state=1)\r\n",
    "                  )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "model"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Conveyor= (\n",
       "           <experiments.exp1.user_transform.UserTransform object at 0x000001C2E0BD0190>, \n",
       "           Word2Vectorization(columns=['purpose_other'], level_formatting=1, epochs=100, min_count=1, window=5, vector_size=20), \n",
       "           CategoricalEncoder(columns=['organization_type_other', 'position_other', 'email', 'ceduc', 'family', 'sstate', 'cgrag', 'income_source_id', 'income_frequency_id', 'has_prior_employment', 'empoyees_count_id', 'organization_branch_id', 'organization_type_id', 'position_id', 'employment_type_id', 'has_movables', 'has_immovables', 'fact_addr_owner_type_id', 'fact_addr_region_id', 'fact_addr_same', 'addr_owner_type_id', 'addr_region_id', 'education_id', 'children_count_id', 'marital_status_id', 'gender_id', 'country_det', 'city_det', 'region_det', 'isp', 'browser', 'system', 'brand']), \n",
       "           ImputerIterative(columns=None, max_iter=10, initial_strategy=mean, missing_values=nan), \n",
       "           ElasticNetCV(l1_ratio=0.05, tol=0.1)\n",
       "            )"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "model.fit(df, df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  3.45it/s, transform=ImputerIterative]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start fit model !!!!\n",
      "BEST TPOT:\n",
      "Pipeline(steps=[('stackingestimator',\n",
      "                 StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.85,\n",
      "                                                                       learning_rate=0.5,\n",
      "                                                                       loss='lad',\n",
      "                                                                       max_depth=1,\n",
      "                                                                       max_features=0.5,\n",
      "                                                                       min_samples_leaf=12,\n",
      "                                                                       min_samples_split=18,\n",
      "                                                                       subsample=0.2))),\n",
      "                ('elasticnetcv', ElasticNetCV(l1_ratio=0.05, tol=0.1))])\n",
      "RESULT CONVEYOR:\n",
      "Conveyor= (\n",
      "           <experiments.exp1.user_transform.UserTransform object at 0x000001C2E0BD0190>, \n",
      "           Word2Vectorization(columns=['purpose_other'], level_formatting=1, epochs=100, min_count=1, window=5, vector_size=20), \n",
      "           CategoricalEncoder(columns=['organization_type_other', 'position_other', 'email', 'ceduc', 'family', 'sstate', 'cgrag', 'income_source_id', 'income_frequency_id', 'has_prior_employment', 'empoyees_count_id', 'organization_branch_id', 'organization_type_id', 'position_id', 'employment_type_id', 'has_movables', 'has_immovables', 'fact_addr_owner_type_id', 'fact_addr_region_id', 'fact_addr_same', 'addr_owner_type_id', 'addr_region_id', 'education_id', 'children_count_id', 'marital_status_id', 'gender_id', 'country_det', 'city_det', 'region_det', 'isp', 'browser', 'system', 'brand']), \n",
      "           ImputerIterative(columns=None, max_iter=10, initial_strategy=mean, missing_values=nan), \n",
      "           ElasticNetCV(l1_ratio=0.05, tol=0.1)\n",
      "            )\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "experiment = Experimenter(\"exp1\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "load model successful!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "description = \\\r\n",
    "\"\"\"\r\n",
    "Иморт user_transform из __main__\r\n",
    "Расширенная модель для новых пользователей, использует только статус 5 и 6.\r\n",
    "Расширения подразумевает исользование user_agent, detection.\r\n",
    "\"\"\"\r\n",
    "testset_name = 'testset_2_5k_y2021_m6_new'\r\n",
    "X_test_features = ['backend_application_id', 'overdue_days','status_id']\r\n",
    "experiment.create_experiment(model, description, 'test', df, df, testset_name, X_test_features = X_test_features, feature_importances = False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "************************************************************\n",
      "testset = testset_2_5k_y2021_m6_new\n",
      "function - roc_auc_score = 0.6094999999999999\n",
      "function - r2_score = 0.030458245523406147\n",
      "function - accuracy_score = ERROR: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "experiment.make_experiment(X_test, y_test, testset_name, X_test_features = X_test_features)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "************************************************************\n",
      "testset = testset_2_5k_y2021_m6_new\n",
      "function - roc_auc_score = 0.8713586751975914\n",
      "function - r2_score = -0.38745845517811994\n",
      "function - accuracy_score = ERROR: Classification metrics can't handle a mix of binary and continuous targets\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from ast import literal_eval\r\n",
    "from typing import List\r\n",
    "import json\r\n",
    "\r\n",
    "def pars_user_agent(user_agent:str):\r\n",
    "    browser, system, brand, = (np.nan, np.nan, np.nan)\r\n",
    "    list_user_agent = []\r\n",
    "    try:\r\n",
    "        with open('user_agent.json', 'r') as load_file:\r\n",
    "            info = pd.DataFrame(json.load(load_file))\r\n",
    "        list_user_agent = info['useragent'].values\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "\r\n",
    "    if user_agent  in list_user_agent:\r\n",
    "        system = info[info['useragent'] == user_agent]['system'].values[0].lower().split(' ')\r\n",
    "        browser = system[0]\r\n",
    "        system = 'windows' if system[2].find('win') != -1 else system[2]\r\n",
    "        brand = 'apple' if (system == 'macos') else np.nan\r\n",
    "    else:\r\n",
    "        user_agent = user_agent[user_agent.index('(')+1:user_agent.index(')')].lower().split(';')\r\n",
    "\r\n",
    "        system = {'windows':('windows', np.nan), 'x11':('linux',np.nan), \r\n",
    "                    'iphone':('iphone', 'apple'), 'ipad':('ipad', 'apple'), 'macintosh':('macos', 'apple')}\r\n",
    "        for key in system.copy().keys():\r\n",
    "            for col in user_agent:\r\n",
    "                if col.find(key) != -1:\r\n",
    "                    system, brand = system[key]\r\n",
    "                    return browser, system, brand \r\n",
    "        else:\r\n",
    "            isandroid = len([i for i in user_agent if i.find('android') != -1]) > 0            \r\n",
    "            if isandroid:\r\n",
    "                brand_phone = {'samsung':'samsung', 'xiaomi':'xiaomi', 'huawei':'huawei', 'lenovo':'lenovo',\r\n",
    "                                'motorola':'motorola', 'nokia':'nokia', 'sony':'sony', 'honor':'huawei', \r\n",
    "                                'tecno':'tecno', 'asus':'asus', 'meizu':'meizu', 'vivo':'vivo', 'neffos':'neffos',\r\n",
    "                                'ulefone':'ulefone', 'htc ':'htc', 'pocophone':'poco', 'pixel':'google',\r\n",
    "                                'lg':'lg', 'sm':'samsung', 'redmi':'xiaomi', 'oneplus':'huawei', 'htc':'htc',\r\n",
    "                                'zte':'zte', 'mi':'xiaomi', 'm200':'xiaomi', 'cph':'oppo', 'moto':'motorola',\r\n",
    "                                'rmx':'realme', 'jsn':'huawei','-lx':'huawei', 'yal-':'huawei', 'eml-':'huawei',\r\n",
    "                                '-l21':'huawei', '-l29':'huawei', '-l22':'huawei', '-l31':'huawei','psp':'prestigio',\r\n",
    "                                '-l09':'huawei', '-l19':'huawei', 'pra-':'huawei', '-l41':'huawei', '-u29':'huawei', \r\n",
    "                                'mz':'meizu', 'u10':'meizu', 'm5':'xiaomi','m6':'xiaomi', 'note':'xiaomi',\r\n",
    "                                }\r\n",
    "                system = 'android'\r\n",
    "                for key in brand_phone.keys():\r\n",
    "                    for col in user_agent:\r\n",
    "                        if col.find(key) != -1:\r\n",
    "                            brand = brand_phone[key]\r\n",
    "                            return browser, system, brand\r\n",
    "                             \r\n",
    "    return [browser, system, brand]\r\n",
    "\r\n",
    "def pars_detections(detections:str):\r\n",
    "    country, region, city, isp = np.nan, np.nan, np.nan, np.nan\r\n",
    "    try:\r\n",
    "        detections = literal_eval(detections)['geo']\r\n",
    "        isp = detections['isp']\r\n",
    "        country = detections['country']\r\n",
    "        city = detections['city']\r\n",
    "        region = int(detections['region'])\r\n",
    "    finally:\r\n",
    "        return [country, city, region, isp]\r\n",
    "\r\n",
    "class UserTransform():\r\n",
    "\r\n",
    "    __name__ = 'pars'\r\n",
    "\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "    def fit(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series):\r\n",
    "        return self\r\n",
    "        \r\n",
    "    def transform(self, X:pd.DataFrame, y = None) -> pd.DataFrame:\r\n",
    "        leave_columns = ['loan_amount', 'loan_days', 'gender_id', 'marital_status_id', 'children_count_id', 'education_id', 'addr_region_id',\r\n",
    "                         'addr_owner_type_id', 'fact_addr_same', 'fact_addr_region_id', 'fact_addr_owner_type_id', 'has_immovables', 'has_movables',\r\n",
    "                         'employment_type_id', 'position_id', 'organization_type_id', 'organization_branch_id', 'empoyees_count_id', 'seniority_years',\r\n",
    "                         'has_prior_employment', 'monthly_income', 'income_frequency_id', 'income_source_id', 'monthly_expenses', 'other_loans_about_current', \r\n",
    "                         'other_loans_about_monthly', 'product_dpr', 'product_amount_from', 'product_amount_to', 'product_overdue_dpr', 'product_interest_min', \r\n",
    "                         'median_day_credit',\t'mean_credit_summ',\t'mean_credit_debt', 'last_cdolgn', 'last_wdohod', 'last_wstag', 'cgrag', 'sstate', 'family', \r\n",
    "                         'ceduc', 'ubki_balance_value', 'ubki_score', 'ubki_scorelast', 'ubki_scorelevel', 'ubki_all_credits', 'ubki_open_credits', \r\n",
    "                         'ubki_closed_credits', 'ubki_expyear', 'ubki_maxnowexp', 'ubki_phone_deltatime', 'ubki_email_deltatime', 'ubki_week_queries',\r\n",
    "                         'rejected_applications_count', 'mean_loans', 'applied_at', 'purpose_other', 'birth_date', 'passport_date', 'email', 'position_other', \r\n",
    "                         'organization_type_other', 'detections', 'user_agent']\r\n",
    "        X = X[leave_columns]\r\n",
    "\r\n",
    "        X = X.replace('[]', np.nan, regex=False)\r\n",
    "        X['email'] = X['email'].str.split('@', expand=True)[1]\r\n",
    "\r\n",
    "        X['passport_year'] = pd.to_datetime(X['passport_date'], format='%Y-%m-%d', errors='coerce').dt.year\r\n",
    "        \r\n",
    "\r\n",
    "        X['birth_year'] = pd.to_datetime(X['birth_date'], format='%Y-%m-%d', errors='coerce').dt.year\r\n",
    "        \r\n",
    "\r\n",
    "        X['applied_at'] = pd.to_datetime(X['applied_at'], format='%Y-%m-%d %H', errors='coerce')\r\n",
    "        X['applied_day'] = X['applied_at'].dt.day\r\n",
    "        X['applied_weekday'] = X['applied_at'].dt.weekday\r\n",
    "        X['applied_hour'] = X['applied_at'].dt.hour\r\n",
    "        \r\n",
    "        X = X.drop(['passport_date', 'birth_date', 'applied_at'], axis = 1)\r\n",
    "\r\n",
    "        X[['country_det', 'city_det', 'region_det', 'isp']] = [pars_detections(val) for val in X['detections']]\r\n",
    "        X[['browser', 'system', 'brand']] = [pars_user_agent(val) for val in X['user_agent']]\r\n",
    "        \r\n",
    "        X = X.drop(['detections', 'user_agent'], axis = 1)\r\n",
    "        return X\r\n",
    "\r\n",
    "    def target_transform(self, Y:pd.DataFrame) -> pd.DataFrame or pd.Series or List[float or int]:\r\n",
    "        Y = Y[['overdue_days', 'status_id']]\r\n",
    "        Y['overdue_days'] = Y['overdue_days'].fillna(0)\r\n",
    "        Y['overdue_days'].loc[Y['overdue_days'] == 0] = 0\r\n",
    "        Y['overdue_days'].loc[Y['overdue_days'] > 0] = 1\r\n",
    "        Y['overdue_days'].loc[Y['status_id'] == 2] = 1\r\n",
    "        return Y['overdue_days'].replace({0: 1, 1: 0})"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3a56291688fe74fff73a1fbf6b0dc2a5cfa3a0985083944054e17fe03253671"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}