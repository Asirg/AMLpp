{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel('check.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(\r\n",
    "    {'fea1':[0,0,0,2,2,2,1,1,1],\r\n",
    "     'fea2':['a',np.nan,'b','b','b','b',np.nan,'c','c'],\r\n",
    "     'fea3':['Магазин','Магазин1','Магази','Что','ЧТОт','ЧТК','ЧАШК','ЧАШКК','чаш']})\r\n",
    "Y = pd.DataFrame({'target':[1,1,1,1,0,1,0,0,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(\r\n",
    "    {'fea1':[0,0,1,2,5],\r\n",
    "     'fea2':['a',np.nan,'b',np.nan,'c'],\r\n",
    "     'fea3':['1','1','1','1','1']})\r\n",
    "Y_test = pd.DataFrame({'target':[1,1,1,1,0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\r\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\r\n",
    "from sklearn.preprocessing import OrdinalEncoder\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.stem import SnowballStemmer\r\n",
    "from gensim.models import Word2Vec\r\n",
    "from typing import List, Callable\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from tpot import TPOTRegressor\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import pymorphy2\r\n",
    "import shelve\r\n",
    "import time\r\n",
    "import shap\r\n",
    "import os \r\n",
    "\r\n",
    "snowball = SnowballStemmer(language=\"russian\")\r\n",
    "morph = pymorphy2.MorphAnalyzer()\r\n",
    "stop_words = stopwords.words(\"russian\")\r\n",
    "############################################################################################################\r\n",
    "\r\n",
    "def most_frequency(x:List[List[int]]):\r\n",
    "    x_ = [i[0] for i in x]\r\n",
    "    return np.argmax(np.bincount(x_))\r\n",
    "\r\n",
    "def lead_time(func):\r\n",
    "    def wrapper(*args, **kwargs):\r\n",
    "        start_time = time.time()\r\n",
    "        result =  func(*args, **kwargs)\r\n",
    "        print('lead time {} = {:.3f}'.format(func.__name__, time.time() - start_time))\r\n",
    "        return result \r\n",
    "    return wrapper\r\n",
    "def shelve_save(save_data:object, key:str, path:str = 'model_property/data'):\r\n",
    "    if not os.path.exists(\"/\".join(path.split('/'))):\r\n",
    "        os.makedirs(\"/\".join(path.split('/')))\r\n",
    "    with shelve.open(path) as save:\r\n",
    "        save[key] = save_data\r\n",
    "\r\n",
    "def shelve_load(key:str, path:str = 'model_property/data'):\r\n",
    "    with shelve.open(path) as load:\r\n",
    "        return load[key]\r\n",
    "############################################################################################################\r\n",
    "from sklearn.metrics import roc_auc_score\r\n",
    "class Conveyor:\r\n",
    "    \"\"\" Подобие sklearn.Pipeline, адаптированный под простоту и добавленный функционал\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    *block : object\r\n",
    "        Объекты классов, что будут использоваться при обработке, и моделирование\r\n",
    "\r\n",
    "    \"\"\"\r\n",
    "    ################################################################\r\n",
    "    def __init__(self, *blocks, **params):\r\n",
    "        self.blocks = list(blocks)\r\n",
    "    \r\n",
    "    @lead_time\r\n",
    "    def fit(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series):\r\n",
    "        X_, Y_  = (X.copy(), Y.copy())\r\n",
    "        for block in self.blocks[:-1]:\r\n",
    "            block.fit(X_, Y_)\r\n",
    "            X_, Y_ = self._transform(block, X_, Y_)\r\n",
    "        self.blocks[-1].fit(X_, Y_)\r\n",
    "        return X_, Y_\r\n",
    "\r\n",
    "    @lead_time\r\n",
    "    def fit_transform(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series):\r\n",
    "        X_, Y_  = (X.copy(), Y.copy())\r\n",
    "        for block in self.blocks:\r\n",
    "            block.fit(X_, Y_)\r\n",
    "            X_, Y_ = self._transform(block, X_, Y_)\r\n",
    "        return X_, Y_\r\n",
    "    ################################################################\r\n",
    "    @lead_time\r\n",
    "    def transform(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series = pd.DataFrame()):\r\n",
    "        X_, Y_  = (X.copy(), Y.copy())\r\n",
    "        for block in self.blocks[:-1]:\r\n",
    "            X_, Y_ = self._transform(block, X_, Y_)\r\n",
    "        return X_, Y_\r\n",
    "\r\n",
    "    def _transform(self, block, X:pd.DataFrame, Y:pd.DataFrame or pd.Series = pd.DataFrame()):\r\n",
    "        X = block.transform(X)\r\n",
    "        if not Y.empty and 'target_transform' in dir(block):\r\n",
    "            Y = block.target_transform(Y)\r\n",
    "        return X, Y\r\n",
    "\r\n",
    "    ################################################################\r\n",
    "\r\n",
    "    @lead_time\r\n",
    "    def predict(self, X:pd.DataFrame):\r\n",
    "        return self.blocks[-1].predict(self.transform(X.copy())[0])\r\n",
    "\r\n",
    "    ################################################################\r\n",
    "    @lead_time\r\n",
    "    def score(self,\r\n",
    "                X:pd.DataFrame,\r\n",
    "                Y:pd.DataFrame or pd.Series,\r\n",
    "                sklearn_function:List[str] = ['roc_auc_score', 'r2_score', 'accuracy_score'],\r\n",
    "                precision_function:List[Callable] = []):\r\n",
    "\r\n",
    "        X_, Y_ = self.transform(X.copy(), Y.copy())\r\n",
    "        result = self.blocks[-1].predict(X_)\r\n",
    "\r\n",
    "        for func in sklearn_function:\r\n",
    "            try:\r\n",
    "                exec('from sklearn.metrics import ' + func)\r\n",
    "                print(\"function - {} = \".format(func), eval(\"{}(result, Y_)\".format(func)))\r\n",
    "            except Exception as e:\r\n",
    "                print(\"function - {} = ERROR: {}\".format(func, e))\r\n",
    "        for func in precision_function:\r\n",
    "            try:\r\n",
    "                print(\"function - {} = \".format(func.__name__), func(result, Y_))\r\n",
    "            except Exception as e:\r\n",
    "                print(\"function - {} = ERROR: {}\".format(func.__name__, e))\r\n",
    "    @lead_time\r\n",
    "    def feature_importances(self,\r\n",
    "                            X:pd.DataFrame,\r\n",
    "                            Y:pd.DataFrame or pd.Series, show:str = 'all'): # all, sklearn, shap\r\n",
    "                            \r\n",
    "        X_, Y_ = self.transform(X.copy(), Y.copy())\r\n",
    "        estimator = self.blocks[-1][-1] if type(self.blocks[-1]) == Pipeline else self.blocks[-1]\r\n",
    "\r\n",
    "        if show == 'all' or show == 'shap':\r\n",
    "            explainer = shap.Explainer(estimator)\r\n",
    "            shap_values = explainer(X_)\r\n",
    "            shap.plots.bar(shap_values[0])\r\n",
    "\r\n",
    "        if show == \"all\" or show == \"sklearn\":\r\n",
    "            try:\r\n",
    "                result = permutation_importance(estimator, X_, Y_, n_repeats=2, random_state=42)\r\n",
    "                index = X_.columns if type(X_) == pd.DataFrame else X.columns\r\n",
    "                forest_importances = pd.Series(result.importances_mean, index=index)\r\n",
    "                fig, ax = plt.subplots(figsize=(20, 10))\r\n",
    "                forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\r\n",
    "                ax.set_title(\"Feature importances using permutation on full model\")\r\n",
    "                ax.set_ylabel(\"Mean accuracy decrease\")\r\n",
    "                fig.tight_layout()\r\n",
    "                plt.show()\r\n",
    "            except Exception as e:\r\n",
    "                print('Sklearn plot - ERROR: ', e)\r\n",
    "    ################################################################\r\n",
    "    @lead_time\r\n",
    "    def fit_model(self, \r\n",
    "                    X:pd.DataFrame, Y:pd.DataFrame or pd.Series,\r\n",
    "                    type_model:str = 'regressor', estimator:bool = False,\r\n",
    "                    generations:int = 5, population_size:int = 50, n_jobs:int = -1):\r\n",
    "\r\n",
    "        tpot = TPOTRegressor(generations=1, population_size=20, n_jobs = -1, random_state=42)\r\n",
    "        if not estimator:\r\n",
    "            X_, Y_ = self.fit_transform(X, Y)\r\n",
    "        else:\r\n",
    "            X_, Y_ = self.fit(X, Y)\r\n",
    "        # X_, Y_ = self.fit_transform(X, Y) if not estimator else self.fit(X, Y)\r\n",
    "        tpot.fit(X_, Y_)\r\n",
    "        make_pipe, import_libs = tpot.export('', get_pipeline=True)\r\n",
    "\r\n",
    "        exec(import_libs)\r\n",
    "        tpot_model = eval(make_pipe)\r\n",
    "        tpot_model = tpot_model if (type(tpot_model) == Pipeline) else make_pipeline(tpot_model)\r\n",
    "\r\n",
    "        if estimator:\r\n",
    "            del self.blocks[-1]\r\n",
    "        \r\n",
    "        for step in tpot_model:\r\n",
    "            self.blocks.append(step)\r\n",
    "            self.blocks[-1].fit(X_, Y_)\r\n",
    "            if step != tpot_model[-1]:\r\n",
    "                X_, Y_ = self._transform(self.blocks[-1], X_, Y_)\r\n",
    "            \r\n",
    "        self.blocks[-1].fit(X_, Y_)\r\n",
    "        print(self.blocks)\r\n",
    "    ################################################################\r\n",
    "    @lead_time\r\n",
    "    def export(self):\r\n",
    "        pass\r\n",
    "\r\n",
    "class CategoricalEncoder():\r\n",
    "    \"\"\" Класс кодирования категориальных данных, с заполнение пропусков на некоторое значение определенное сратегией\r\n",
    "\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    columns : List[str]\r\n",
    "        Названия столбцов, которые будут подвегнуты обработке\r\n",
    "\r\n",
    "    straegy : str\r\n",
    "        Строка указывающая на используемую стратегию заполнения пропусков\r\n",
    "    \r\n",
    "    fill_value : float or str\r\n",
    "        Заполнитель, которым будут заполняться пропущенные значения,\r\n",
    "        при использование стратегии const\r\n",
    "        \r\n",
    "    \"\"\"\r\n",
    "    encoder = {}\r\n",
    "\r\n",
    "    def __init__(self, columns:List[str], strategy:str='mean', fill_value:float or str = np.nan): # strategy in mean, median, most_frequency, const, iterative inputer?\r\n",
    "        self.columns = columns\r\n",
    "        self.fill_value = {'mean':np.mean, 'median':np.median, 'most_freq':most_frequency, 'const':(lambda x:fill_value)}\r\n",
    "        self.fill_value = self.fill_value[strategy]\r\n",
    "\r\n",
    "    def fit(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series):\r\n",
    "        for column in self.columns:\r\n",
    "            self.encoder[column] = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\r\n",
    "            X_fit = pd.DataFrame(X[column].loc[~X[column].isnull()])\r\n",
    "            self.encoder[column].fit(X_fit)\r\n",
    "            X_transform = self.encoder[column].transform(pd.DataFrame(X_fit))\r\n",
    "            self.encoder[column].unknown_value = self.fill_value(X_transform)\r\n",
    "        shelve_save(self.encoder, 'CategoricalEncoder')\r\n",
    "        return self\r\n",
    "    def transform(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series = None):\r\n",
    "        self.encoder = shelve_load('CategoricalEncoder')\r\n",
    "        for column in self.columns:\r\n",
    "            X[column] = self.encoder[column].transform(pd.DataFrame(X[column].fillna('NAN')))\r\n",
    "        return X\r\n",
    "\r\n",
    "class Word2Vectorization():\r\n",
    "\r\n",
    "    len_sentence = {} \r\n",
    "    mean_word = {} \r\n",
    "    word2 = {} \r\n",
    "\r\n",
    "    def __init__(self, columns:List[str], level_formatting:int = 0):\r\n",
    "        self.columns = columns\r\n",
    "\r\n",
    "    def fit(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series):\r\n",
    "        for column in self.columns: \r\n",
    "            filtered = [self.refactor_string(str(i)) for i in X[column]]\r\n",
    "            self.word2[column] = Word2Vec(sentences=filtered, epochs=5000, \r\n",
    "                                    min_count=1, window=5, vector_size=1,\r\n",
    "                                    sg=1, cbow_mean=1, alpha=0.1,\r\n",
    "                                    seed=self.seed)\r\n",
    "\r\n",
    "            word_vac = [self.word2[column].wv[i] for i in self.word2[column].wv.key_to_index.values()]\r\n",
    "            self.len_sentence[column] = max([len(sentence) for sentence in filtered]) \r\n",
    "            self.mean_word[column] = np.mean(word_vac)                              \r\n",
    "        return self\r\n",
    "    def transform(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series = None):\r\n",
    "        \r\n",
    "        return X\r\n",
    "\r\n",
    "    def refactor_string(self, string:str)->List[str]:\r\n",
    "        string = string if str(string) != 'nan' else \"\"                          # Проверка на NAN\r\n",
    "        string = word_tokenize(str(string).lower())                              # Нижний регистр и токенизация\r\n",
    "        if self.level_formatting > 0:\r\n",
    "            string = [i for i in string if i.isalpha()]                              # Избавления от знаков пунктуации\r\n",
    "            if self.level_formatting > 1:\r\n",
    "                string = [i for i in string if not i in stop_words]                      # Избавления от стоп слов\r\n",
    "                if self.level_formatting > 2:\r\n",
    "                    string = [snowball.stem(morph.parse(i)[0].normal_form) for i in string]  # СТЭММИНГ и ЛЕММАТИЗАЦИЯ\r\n",
    "        return string\r\n",
    "\r\n",
    "    def mean_word2vec(self, sentence:str, column:str) ->List[float]:\r\n",
    "        vector = self.refactor_string(sentence)\r\n",
    "        vector = [self.word2[token] for token in vector if token in self.word2.wv.key_to_index.keys()]\r\n",
    "        return np.mean(vector) if vector != [] else 0\r\n",
    "################################################################################################\r\n",
    "    ## Выборка слов из датасета, и подача их на вход \r\n",
    "    def fit(self, X:pd.DataFrame, y:pd.Series or List[float]):\r\n",
    "        for column in self.columns: \r\n",
    "            filtered = [self.refactor_string(str(i)) for i in X[column]]\r\n",
    "            self.word2[column] = Word2Vec(sentences=filtered, epochs=5000, \r\n",
    "                                    min_count=1, window=5, vector_size=1,\r\n",
    "                                    sg=1, cbow_mean=1, alpha=0.1,\r\n",
    "                                    seed=self.seed)\r\n",
    "\r\n",
    "            word_vac = [self.word2[column].wv[i] for i in self.word2[column].wv.key_to_index.values()]\r\n",
    "            self.len_sentence[column] = max([len(sentence) for sentence in filtered]) \r\n",
    "            self.mean_word[column] = np.mean(word_vac)                                  \r\n",
    "\r\n",
    "        if not os.path.exists('model_new_property'):\r\n",
    "            os.makedirs('model_new_property')\r\n",
    "        with open('model_new_property/len_sentence', 'wb') as file:\r\n",
    "            pickle.dump(self.len_sentence, file)\r\n",
    "        with open('model_new_property/mean_word', 'wb') as file:\r\n",
    "            pickle.dump(self.mean_word, file)\r\n",
    "        with open('model_new_property/word2', 'wb') as file:\r\n",
    "            pickle.dump(self.word2, file)\r\n",
    "\r\n",
    "        return self\r\n",
    "\r\n",
    "    def transform(self, X:pd.DataFrame, y = None)->pd.Series:\r\n",
    "        with open('model_new_property/len_sentence', 'rb') as file:\r\n",
    "            self.len_sentence = pickle.load(file)\r\n",
    "        with open('model_new_property/mean_word', 'rb') as file:\r\n",
    "            self.mean_word = pickle.load(file)\r\n",
    "        with open('model_new_property/word2', 'rb') as file:\r\n",
    "            self.word2 = pickle.load(file)\r\n",
    "\r\n",
    "        for column in self.columns:\r\n",
    "            X[column] = list([self.get_vector_sentence(val, column) for val in X[column]])\r\n",
    "        return X\r\n",
    "\r\n",
    "# class Imputer():\r\n",
    "class user_transform():\r\n",
    "    def __init__(self):\r\n",
    "        pass\r\n",
    "    def fit(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series):\r\n",
    "        return self\r\n",
    "    def transform(self, X:pd.DataFrame, y = None) -> pd.DataFrame:\r\n",
    "        X = X[['closed_credits_count', 'ubki_week_queries','loan_amount','loan_days',\r\n",
    "        'ubki_email_deltatime','ubki_phone_deltatime','ubki_maxnowexp','ubki_expyear',\r\n",
    "        'marital_status_id']]\r\n",
    "        X = X.fillna(0)\r\n",
    "        return X\r\n",
    "\r\n",
    "    def target_transform(self, dataset:pd.DataFrame) -> pd.Series or List[float or int]:\r\n",
    "        y = dataset['user_id']\r\n",
    "        return y\r\n",
    "    # def fit(self, X:pd.DataFrame, Y:pd.DataFrame or pd.Series):\r\n",
    "    #     return self\r\n",
    "\r\n",
    "    # def transform(self, X:pd.DataFrame):\r\n",
    "    #     return X\r\n",
    "    \r\n",
    "    # def target_transform(self, Y:pd.DataFrame or pd.Series):\r\n",
    "    #     Y['target'] = [Y.loc[i, 'target']*-1 for i in range(len(Y['target']))]\r\n",
    "    #     return Y\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\r\n",
    "from sklearn.pipeline import make_pipeline\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\r\n",
    "from sklearn.kernel_approximation import RBFSampler\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.pipeline import make_pipeline, make_union\r\n",
    "from tpot.builtins import StackingEstimator\r\n",
    "from tpot.export_utils import set_param_recursive \r\n",
    "\r\n",
    "pipe1 = make_pipeline(StandardScaler())\r\n",
    "\r\n",
    "model = Conveyor(user_transform(),\r\n",
    "                CategoricalEncoder(columns=['marital_status_id']),\r\n",
    "                # pipe1,\r\n",
    "                # pipe2\r\n",
    "                RandomForestRegressor()\r\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead time fit = 0.222\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model.fit(X, Y)\r\n",
    "model.fit(dataset[:60], dataset[:60])\r\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead time fit = 0.156\n",
      "[<__main__.user_transform object at 0x000002255BF0B130>, <__main__.CategoricalEncoder object at 0x000002255BF0B4F0>, Binarizer(threshold=0.65), StackingEstimator(estimator=ElasticNetCV(l1_ratio=0.8500000000000001, tol=0.1)), RidgeCV(alphas=array([ 0.1,  1. , 10. ]))]\n",
      "lead time fit_model = 6.999\n"
     ]
    }
   ],
   "source": [
    "model.fit_model(dataset[:60], dataset[:60], estimator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.user_transform at 0x2255bf0b130>,\n",
       " <__main__.CategoricalEncoder at 0x2255bf0b4f0>,\n",
       " Binarizer(threshold=0.65),\n",
       " RidgeCV(alphas=array([ 0.1,  1. , 10. ]))]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead time transform = 0.007\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The passed model is not callable and cannot be analyzed directly with the given masker! Model: RidgeCV(alphas=array([ 0.1,  1. , 10. ]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-153-fbd8a06d4172>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-145-63006b64ebb4>\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lead time {} = {:.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-145-63006b64ebb4>\u001b[0m in \u001b[0;36mfeature_importances\u001b[1;34m(self, X, Y, show)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mshow\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'all'\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mshow\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'shap'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0mexplainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[0mshap_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mshap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\shap\\explainers\\_explainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, masker, link, algorithm, output_names, feature_names, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;31m# if we get here then we don't know how to handle what was given to us\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The passed model is not callable and cannot be analyzed directly with the given masker! Model: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;31m# build the right subclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: The passed model is not callable and cannot be analyzed directly with the given masker! Model: RidgeCV(alphas=array([ 0.1,  1. , 10. ]))"
     ]
    }
   ],
   "source": [
    "model.feature_importances(dataset[40:], dataset[40:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\r\n",
    "with open('model_' , 'wb') as save_model:\r\n",
    "    pickle.dump(model, save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3a56291688fe74fff73a1fbf6b0dc2a5cfa3a0985083944054e17fe03253671"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}